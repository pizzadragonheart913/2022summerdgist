# 발표자료

### 1. 퍼셉트론
- 퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호로 출력함.
- 각 입력에 가중치를 곱하고 더함. 이 결과에 따라 출력을 정함.

![퍼셉트론](perceptron.PNG)

- 일반 퍼셉트론의 한계는 XOR같은 비선형 영역을 표현할 수 없음.
- 층을 쌓아 다층 퍼셉트론으로 구현하면 비선형 영역을 표현할 수 있음.

### 2. 신경망
- 신경망은 이전의 퍼셉트론에서 하던 가중치 매개변수를 스스로 학습하는 성질을 지님.

![신경망](신경망.png)

- 각 신호에 가중치를 곱한것과 편향을 합쳐 활성화 함수 h(x)를 만듦.
- 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지 정하는 역할을 함. ex) 계단함수, 시그모이드, 렐루
- 하지만 시그모이드와 계단함수는 은닉층의 깊이가 깊으면 오차율을 계산하기 어려워짐. (gradient vanishing problem의 발생)
- gradient vanishing을 해결하기위해 렐루의 등장

![렐루](렐루.png)

### 3. 다차원 배열
- 입력신호에 다양한 가중치를 곱하여 은닉층의 입력으로 주려면 행렬의 곱연산이 제격.
- 행렬의 곱연산에서는 첫번째 행렬의 행과 두번째 행렬의 열의 길이가 같아야 함.
- 결과물은 첫번째 행렬의 열 BY 두번째 행렬의 행의 크기를 가짐.

### 4. 소프트맥스란
![softmax](%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.PNG)
- 하지만 지수함수가 분자에 존재하여 연산시 불안정 해짐.
- 그러므로 소프트맥스의 입력 중 최댓값을 뺴주면 출력이 0에서 1 사이의 실수로 변함.
- 소프트맥스의 출력을 확률로 해석할 수 있기 때문에 분류 문제에서 매우 강해짐.

### 5. 배치와 에포크
![배치와 에포크](배치처리.PNG)
- 전부 업로드 하는 방법도 있지만 배치로 나누어서 학습하면 메모리 관리에 도움이 됨.

### 6. 신경망 학습
- 훈련데이터와 시험데이터로 나눠 학습
- 훈련데이터에만 과도하게 최적화되는 현상인 오버피팅을 피해야 함

### 7. 손실함수
- 출력과 정답을 비교해서 추정값과 정답의 차를 제곱한 후 총합을 구한것이 SSE임.
![오차제곱합](CCE.png)
- 교차 엔트로피 오차는 정답을 옳게 추정했을때 더 낮은 수치를 보여줌.
![SSE](SSE.png)

### 8. 정확도 VS 손실함수
- 직관적인 정확도 대신 손실함수를 사용하는 이유는 미분을 통해 손실함수의 값을 가장 작게하는 매개변수 값을 찾을 수 있기 때문이다.(경사하강법의 제일 낮은 부분)
- 정확도를 지표로 사용하게 되면 대부분의 지점에서 미분값이 0이 되어버리기 때문에 정확도를 사용할 수 없음.

### 9. 경사법
- 미분의 원리를 이용해서 현위치에서 기울어진 방향으로 일정거리만큼 이동하는것을 반복한다.
- 학습률 설정이 중요함. 현위치에서 얼마만큼 이동할지 보폭의 단위라고 생각하면 쉬움.
- 학습률이 너무 낮으면 갱신이 안되고, 학습률이 너무 크면 오히려 발산하는경우가 생김.

