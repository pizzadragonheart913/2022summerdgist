# 발표자료

### 1. 퍼셉트론
- 퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호로 출력함.
- 각 입력에 가중치를 곱하고 더함. 이 결과에 따라 출력을 정함.

![퍼셉트론](perceptron.PNG)

- 일반 퍼셉트론의 한계는 XOR같은 비선형 영역을 표현할 수 없음.
- 층을 쌓아 다층 퍼셉트론으로 구현하면 비선형 영역을 표현할 수 있음.

### 2. 신경망
- 신경망은 이전의 퍼셉트론에서 하던 가중치 매개변수를 스스로 학습하는 성질을 지님.

![신경망](신경망.png)

- 각 신호에 가중치를 곱한것과 편향을 합쳐 활성화 함수 h(x)를 만듦.
- 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지 정하는 역할을 함. ex) 계단함수, 시그모이드, 렐루
- 하지만 시그모이드와 계단함수는 은닉층의 깊이가 깊으면 오차율을 계산하기 어려워짐. (gradient vanishing problem의 발생)
- gradient vanishing을 해결하기위해 렐루의 등장

![렐루](렐루.png)

### 3. 다차원 배열
- 입력신호에 다양한 가중치를 곱하여 은닉층의 입력으로 주려면 행렬의 곱연산이 제격.
- 행렬의 곱연산에서는 첫번째 행렬의 행과 두번째 행렬의 열의 길이가 같아야 함.
- 결과물은 첫번째 행렬의 열 BY 두번째 행렬의 행의 크기를 가짐.

### 4. 소프트맥스란
![softmax](%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.PNG)
- 하지만 지수함수가 분자에 존재하여 연산시 불안정 해짐.
- 그러므로 소프트맥스의 입력 중 최댓값을 뺴주면 출력이 0에서 1 사이의 실수로 변함.
- 소프트맥스의 출력을 확률로 해석할 수 있기 때문에 분류 문제에서 매우 강해짐.

### 5. 배치와 에포크
![배치와 에포크](배치처리.PNG)
- 전부 업로드 하는 방법도 있지만 배치로 나누어서 학습하면 메모리 관리에 도움이 됨.

### 6. 신경망 학습
- 훈련데이터와 시험데이터로 나눠 학습
- 훈련데이터에만 과도하게 최적화되는 현상인 오버피팅을 피해야 함

### 7. 손실함수
- 출력과 정답을 비교해서 추정값과 정답의 차를 제곱한 후 총합을 구한것이 SSE임.
![오차제곱합](CCE.png)
- 교차 엔트로피 오차는 정답을 옳게 추정했을때 더 낮은 수치를 보여줌.
![SSE](SSE.png)

### 8. 정확도 VS 손실함수
- 직관적인 정확도 대신 손실함수를 사용하는 이유는 미분을 통해 손실함수의 값을 가장 작게하는 매개변수 값을 찾을 수 있기 때문이다.(경사하강법의 제일 낮은 부분)
- 정확도를 지표로 사용하게 되면 대부분의 지점에서 미분값이 0이 되어버리기 때문에 정확도를 사용할 수 없음.

### 9. 경사법
- 미분의 원리를 이용해서 현위치에서 기울어진 방향으로 일정거리만큼 이동하는것을 반복한다.
- 학습률 설정이 중요함. 현위치에서 얼마만큼 이동할지 보폭의 단위라고 생각하면 쉬움.
- 학습률이 너무 낮으면 갱신이 안되고, 학습률이 너무 크면 오히려 발산하는경우가 생김.

### 10. 오차 역전파법
- 가중치 매개변수의 기울기를 효과적으로 계산할 수 있음
- 기존의 수치미분법은 미분을 수행해 비교한 후 낮으면 학습률 만큼 이동하던 과정
- 역전파의 과정 - 신경망의 출력과 정답레이블의 차이를 앞 계층에 전함.

### 11. 확률적 경사하강법 (9. 경사법 보다 더 진보함.)
- 경사하강법은 전체 데이터를 따져서 기울기를 계산.
- 하지만 확률적 경사하강법은 매 step에서 하나의 샘플만 기울기를 계산함. 
- 그러므로 속도에 있어서 더 빠르다는 강점이 있음
- 하지만 노이즈가 커지게 됨.

### 12. 여러가지 경사법 (모멘텀, 에이다그라드, 아담)
- 모멘텀은 가중치를 수정하기 전 이전 방향을 참고해서 같은 방향으로 일정비율만큼 수정되게 함
- 에이다그라드는 많이 변한 변수는 학습률을 낮춰주고, 적게 변한 변수는 학습률을 키움. (능동적으로 보폭 조절)
- 아담 추가하기

### 13. 가중치의 초깃값
- 가중치의 초깃값을 설정하는 것도 매우 중요함
- 만약 전부 0으로 한다면? 학습이 하나도 안될것. 왜냐면 오차역전파과정에서 전부 모든 가중치들이 똑같이 업데이트 되기 때문에 일부러 가우시안 랜덤변수로 마구 흩뿌림.
- 표준편차가 1인 랜덤분포로 초기화 한다면 시그모이드같은 활성화 함수 때문에 0과 1로 치우쳐져 gradient vanishing이 발생함. 기울기 소실은 매우 큰 문제를 일으킴.
- 하비에르(xavier) 초깃값 : 표준편차를 sqrt(1/n)으로 정하고 분포시킴. 여기에서 n은 앞 노드의 갯수 시그모이드나 계단함수일때 사용하면 좋음.
- 히(he) 초깃값 : 표준편차를 sqrt(2/n)으로 정하고 분포시킴. 렐루 함수가 활성화 함수일때 힘을 냄.

### 14. 배치정규화
- 가중치 초깃값 단계에서 활성화 값을 보며 학습이 원활하게 수행되는 것을 봄. 
- 매 층마다 이렇게 퍼뜨리면 어떻게 될까? --> 상당히 좋은 결과를 보여줌.
- 학습 속도 개선, 초깃값에 크게 의존하지 않음, 오버피팅 억제.
- 어파인 계층과 활성화계층 사이에 배치정규화 계층을 삽입.

### 15. 드롭아웃
- 드롭아웃이란 뉴런을 임의로 삭제하면서 학습하는 방법.
- 드롭아웃을 왜 할까? 오버피팅을 막기에 효과적이기 때문.


### 16. 합성곱 신경망
- 합성곱(convolution)이란? 두 함수가 있을 때 하나의 함수를 반전시키고 이동하면서 적분하는 기법.
- 구조는 conv, relu, pooling, conv, relu, pooling, conv, relu, affine, softmax 로 구성
- 